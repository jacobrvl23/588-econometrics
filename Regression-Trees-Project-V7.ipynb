{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression Trees\n",
    "### Economics 588\n",
    "##### Jacob Van Leeuwen, John Bonney, Erik Webb, Taylor Landon, Rachel Bagnall, Scott Elliott, Jaimie Choi, Isaac Riley"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Illustration - Medical Diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Imagine a physician evaluating a potential medical diagnosis for a patient. How can she take advantage of machine learning in an intuitive way without relying on a black-box algorithm? Regression trees can act as an effective decision-making mechanism that provide adequate classification accuracy and a simple representation of gathered knowledge.\n",
    "\n",
    "Suppose the physician is considering a type 2 diabetes diagnosis and that there are two key medical tests: a glycated hemoglobin test and a fasting blood sugar test. The physician has access to the dataset generated below, which contains a diabetes indicator variable and the patients corresponding test result values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-9b0cc929e05a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# https://pythonprogramminglanguage.com/decision-tree-visual-example/\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "# References\n",
    "# https://www.mayoclinic.org/diseases-conditions/type-2-diabetes/diagnosis-treatment/drc-20351199\n",
    "# https://pythonprogramminglanguage.com/decision-tree-visual-example/\n",
    "\n",
    "import graphviz \n",
    "from sklearn import tree\n",
    "import random\n",
    "import decimal\n",
    "import pandas as pd\n",
    "\n",
    "glycated_hemoglobin_test_normal = []\n",
    "glycated_hemoglobin_test_diabetes = []\n",
    "fasting_blood_sugar_test_normal = []\n",
    "fasting_blood_sugar_test_diabetes = []\n",
    "\n",
    "for i in range(0, 500):\n",
    "    # Glycated Hemoglobin Test\n",
    "    glycated_hemoglobin_test_normal.append(float(decimal.Decimal(random.randrange(0, 640))/100))\n",
    "    glycated_hemoglobin_test_diabetes.append(float(decimal.Decimal(random.randrange(570, 800))/100))\n",
    "    # Fasting Blood Sugar Test\n",
    "    fasting_blood_sugar_test_normal.append(float(decimal.Decimal(random.randrange(500, 1050))/100))\n",
    "    fasting_blood_sugar_test_diabetes.append(float(decimal.Decimal(random.randrange(1000, 1260))/100))\n",
    "    \n",
    "glycated_hemoglobin_test = glycated_hemoglobin_test_normal + glycated_hemoglobin_test_diabetes\n",
    "fasting_blood_sugar_test = fasting_blood_sugar_test_normal + fasting_blood_sugar_test_diabetes\n",
    "\n",
    "diabetes_dummy = ([0] * 500) + ([1]*500)\n",
    "d = {'Glycated Hemoglobin':glycated_hemoglobin_test,\n",
    "     'Fasting Blood Sugar':fasting_blood_sugar_test,\n",
    "     'Diabetes': diabetes_dummy}\n",
    "df = pd.DataFrame(d)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Armed with this dataset, she could fit the following decision tree classification model, specifying that the tree  grow only two layers deep so that she can clearly communicate the diagnosis to the patient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-45819b82e2e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Diabetes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Fasting Blood Sugar'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Glycated Hemoglobin'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "Y = df['Diabetes']\n",
    "X = df[['Fasting Blood Sugar', 'Glycated Hemoglobin']]\n",
    "clf = tree.DecisionTreeClassifier(max_depth = 2)\n",
    "clf = clf.fit(X,Y)\n",
    "\n",
    "# Visualize the Tree\n",
    "dot_data = tree.export_graphviz(clf, feature_names = ['Fasting Blood Sugar', 'Glycated Hemoglobin'], \n",
    "                                label = 'root', \n",
    "                                leaves_parallel = True, \n",
    "                                out_file = None, \n",
    "                                impurity = False, \n",
    "                                filled=True, \n",
    "                                rounded=True, \n",
    "                                rotate = False, \n",
    "                                class_names = True)\n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The color of the leaf nodes correspond to the majority class within that node; here the leaf nodes colored orange contain a majority of the non-diabetes class and those colored blue contain a majority of the diabetes class. Suppose the patient's test results indicated a fasting blood sugar level of 10.02 and a glycated hemoglobin value of 4.3. This simple decision tree would predict that the patient does not have diabetes. \n",
    "\n",
    "Although there are other machine learning algorithms that predict with superior accuracy, the true strength of regression trees lies in their visual nature. We will demonstrate another example of this algorithm later on by predicting the selling price of single family homes using variables such as square footage and number of bedrooms. Here is that example, as well as two others:\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "Say we want to predict selling prices of single family homes *(a continuous variable)*. Regression trees can predict this by examining:\n",
    "* Is increased square footage related to prices for single family homes? *(continuous)*\n",
    "* How much is the style of home related to the selling price of single family homes? *(categorical)*\n",
    "* Zip code/county/state/etc. *(categorical)*\n",
    "* Median income of neighborhood/zip code (if area variable is larger than zip code) *(continuous)*\n",
    "\n",
    "Health (Type II Diabetes) *(categorical)*\n",
    "* Does increasing sugar Consumption (avg. grams per day) related to whether or not you develop type II diabetes? *(continuous)* \n",
    "* How does increasing weight affect relate you developing type II diabetes? *(continuous)*\n",
    "* Number of days per week with greater than 30 minutes of exercise *(categorical)*\n",
    "* Age *(continuous)*\n",
    "* Parent has diabetes *(categorical)* \n",
    "* Hours worked/week *(continuous)*\n",
    "\n",
    "Election outcomes (voter-share) *(continuous)*\n",
    "* What State/Region tends to have greater voter-share? *(categorical)*\n",
    "* How much is campaign spending related to voter-share? *(continuous)*\n",
    "* Incumbent *(categorical)*\n",
    "* Political Party *(categorical)*\n",
    "* GDP Growth *(continuous)*\n",
    "* General vs. Midterm Election *(categorical)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Prediction trees are a particular kind of nonlinear predictive model. There are two varieties: regression trees and classification trees.  We will be focused on regression trees. Using linear regressions, we are able to make quantitative predictions. However, linear regressions do not do well with nonlinear models. A solution to this problem can be to partition the data into smaller regions that have more manageable linear interactions. We can recursively subdivide the partitions until we get extremely manageable pieces that can be estimated with simple regression models. This process is known as recursive partitioning. Hence, we use recursive partitioning to sort the data into small, manageable sections and then use a simple model for each part of the partition. \n",
    "\n",
    "A regression tree is a represention of the recursive partitioning process. The basic idea behind regression trees is that each good factor (variable in ML) can be used to make a \"decision\" about the likelihood of an outcome. Each split is called a _node_. The following diagram gives an example:\n",
    "\n",
    "<img src=\"img1.png\">\n",
    "\n",
    "\n",
    "As you can see, the starting point of a tree is called a root node. From there, different branches take us to intermediate nodes called internal nodes, or child nodes. Branches would continue to connect us to intermediate nodes until we reach the end. The last nodes are called leaf nodes, or terminal nodes. Thus, each leaf node of the regression tree represents a part of the partition that has an estimate found using a simple model. The estimate at the leaf nodes applies only to the specific partition. \n",
    "\n",
    "We navigate the tree by asking a sequence of questions about specific features for some observation, $x$. Each question, usually refers to only a single attribute with a yes or no answer. For example, a question of the type could concern gender of the observation (i.e. is the observation male or not). The variables can be either continuous or discrete (but ordered). \n",
    "\n",
    "For classic regression trees, the model in each node is a constant estimate of $Y$. That is, suppose the points $$(x_1,y_1), (x_2,y_2), …, (x_c,y_c)$$ are all the observations belonging to the node, $z$. Then our model for $z$ is: $$\\hat{y}=\\frac{1}{c} \\sum_{i=1}^{c}y_i$$ This is the sample mean of the dependent variable in that node. This is a piecewise-constant model.\t\t\t\t\t\n",
    "\n",
    "One of the problems with recursive partitioning is that we need to balance the informativeness of the partitions with parsimony, so as to not just put every point in its own partition. Similarly, we could just end up putting every point in its own leaf-node, which would not be very useful. A typical stopping criterion is to stop growing the tree when further splits gives less than some minimal amount of extra information, or when they would result in nodes containing less than a small percentage of the total data.  \n",
    "\n",
    "Regression trees can be used to address problems in which we want to predict the value of a continuous variable from a set of continuous and/or categorical variables. Further, if we have enough data, we can split the data into a training and test set, allowing us to predict outcomes given new (similar) data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The goal of the regression tree model is to make the best prediction possible. However, in the regression tree model, we are minimizing the sum of squared residuals for a given tree $T$.\n",
    "The sum of squared residuals for a tree $T$ is $$S=\\sum_{c\\in terminal nodes(T)}\\sum_{i\\in C}(y_i-m_c)^2$$ where $$m_c=\\frac{1}{n_c}\\sum_{i\\in C}y_i$$ is the prediction for leaf $c$. We make our splits to minimize $S$, subject to specified hyperparameters $q$ (the minimum amount of points allowed in each leaf) and $\\delta$ (a lower bound for the largest decrease in $S$).\n",
    "\n",
    "The Algorithm:\n",
    "1. Start with a single node containing all points. Calculate $m_c$ and $S$. \n",
    "2. If all the points in the node have the same value for all the independent variables, stop. Otherwise, search over all binary splits of all variables for the one which will reduce $S$ as much as possible. IF the largest decrease in $S$ would be less than our threshold $\\delta$, or one of the resulting nodes would contain less than $q$ points, stop. Otherwise, take that split, creating two new nodes.\n",
    "3. In each new node, go back to step 1. \n",
    "\n",
    "This will create the tree that minimizes MSE across the leaves. However, this algorithm alone often leads to overfitting, which is a major concern with regression trees - you might get great scores within your training set, but then find that it generalizes poorly. As is often the case with decision trees, there is a tradeoff between bias, variance, and overfitting. The shallower the tree, the greater the bias and the variance, but this may be preferable to overfitting.\n",
    "\n",
    "There are two potential solutions to this problem.\n",
    "\n",
    "The first is called **pruning** -- we grow the largest tree possible before “pruning” it down. To do this, we randomly divide our data into a training set and a testing set, (say, 50% training and 50% testing). We then apply the basic tree-growing algorithm to the training data only, with $q = 1$ and $\\delta = 0$ to grow the largest tree we can. At this point, there is a big overfitting problem, so we prune the tree: at each pair of terminal nodes with a common parent, we evaluate the error on the testing data, and see whether the sum of squares would be smaller by remove those two nodes and making their parent a terminal node. This process is repeated until pruning can no longer improve the error on the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The second potential solution is to use **cross-validation** to choose the hyperparameters for the model. The most common hyperparameters you can specify in a regression tree include: \n",
    "\n",
    "- max depth (how many levels of branches are permitted)\n",
    "- max features (maximum amount of features considered when deciding each split)\n",
    "- min_samples_split (minimum amount of observations per split)\n",
    "- min_samples_leaf (minimum amount of observations at each leaf)\n",
    "- min_weight_fraction_leaf (minimum weighted fraction of all observations per leaf node)\n",
    "- max_leaf_nodes (maximum number of leaf nodes)\n",
    "\n",
    "While both methods are effective, for the technical examples in this paper, we choose to use the cross-validation technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Source: https://clearpredictions.com/Home/DecisionTree\n",
    "Image('tree-infographic.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key Concept: Gini Impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For now, we will start with the simplest case: a classification problem with two outcomes. A common example uses a dataset of passengers on the Titanic to predict who survives.\n",
    "\n",
    "Ideally, we want factors that are as predictive as possible. If men and women are equally likely to survive, the variable can't tell us much (barring interaction with other variables). Fortunately (depending on your perspective, but at least for prediction purposes), it turns out women are more likely than men to survive, so _sex_ will be an important factor in our tree.\n",
    "\n",
    "That means that a node splitting on _sex_ has relatively low Gini impurity. Gini impurity measures the frequency of mislabeling a randomly selected element if it was randomly labeled by the distributions of labels in the subset. A factor with low impurity is very predictive of the outcome. Conversely, the impurity of a node would be maximized if equal proportions of its values (males and females here) survived.\n",
    "\n",
    "Gini impurity is formally defined as:\n",
    "\n",
    "$$Gini_{i} = 1 - \\sum_{k=1}^{n}{p_{i,k}^2}$$\n",
    "\n",
    "For example, if it were the case that 70% of the survivors were females, the Gini impurity of the _sex_ node would be: \n",
    "\n",
    "$$1-0.3^2-0.7^2 = 0.42$$\n",
    "\n",
    "Notice that 0.42 is  probability of mislabelling \n",
    "\n",
    "Now suppose that 50% of the survivors were males, the Gini impurity of the _sex_ node would be: \n",
    "\n",
    "$$1-0.5^2-0.5^2 = 0.5$$\n",
    "\n",
    "Notice here that the Gini impurity probability increased when splitting gender by 50%.\n",
    "\n",
    "Finally, suppose that only 10% of the survivors were males\n",
    "\n",
    "$$1-0.9^2-0.1^2 = 0.18$$\n",
    "\n",
    "Here, the huge disproportionate categorization allows Gini index to be very low, suggesting very low impurity \n",
    "\n",
    "In general, it makes sense to grow a tree **greedily** - starting with the lowest impurest feature splits, then moving to the next lowest impurest.\n",
    "\n",
    "If the dataset has two classes and 50% of the dataset belongs to one class and 50% to another, there is a perfect split and the Gini index is at a maximum. Conversely, if the dataset has two class and all of the instances belong to a single class, the Gini index is at at a minimum, as shown in the image below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Source: http://queirozf.com/entries/evaluation-metrics-for-classification-quick-examples-references\n",
    "from IPython.display import Image\n",
    "Image(\"https://i.imgur.com/DBxpMwl.png\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Under the Hood: Step-by-Step Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now that we have seen a regression tree in action, we can (if time permits) step back and look at what is actually happening when we run a regression tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here are the functions we will create and what they will do:\n",
    "    \n",
    "* test_split() -  Split a dataset based on an attribute and an attribute value\n",
    "* gini_index() -  Calculate the Gini index for a split dataset\n",
    "* get_split() -   Select the best split point for a dataset\n",
    "* to_terminal() - Create a terminal node value\n",
    "* split() -       Create child splits for a node or make terminal\n",
    "* build_tree() -  Build a decision tree\n",
    "* print_tree() -  Print a decision tree\n",
    "* predict() -     Make a prediction with a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def test_split(index, value, dataset):\n",
    "    '''\n",
    "    split a dataset based on an attribute and an attribute value\n",
    "    '''\n",
    "    left, right = list(), list()\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def gini_index(groups, classes):\n",
    "    '''\n",
    "    calculate the gini index for some split\n",
    "    '''\n",
    "    \n",
    "    # count all samples at split point\n",
    "    n_instances = float(sum([len(group) for group in groups]))\n",
    "    # sum weighted Gini index for each group\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        # avoid divide by zero\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        # score the group based on the score for each class\n",
    "        for class_val in classes:\n",
    "            p = [row[-1] for row in group].count(class_val) / size\n",
    "            score += p * p\n",
    "        # weight the group score by its relative size\n",
    "        gini += (1.0 - score) * (size / n_instances)\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def get_split(dataset):\n",
    "    '''\n",
    "    select the best split point for a dataset\n",
    "    '''\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    for index in range(len(dataset[0])-1):\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            gini = gini_index(groups, class_values)\n",
    "        try:\n",
    "            print('X%d < %.3f  =>  Gini = %.3f' % ((index+1), row[index], gini))\n",
    "        except:\n",
    "            print('X%s < %.3f  =>  Gini = %.3f' % ((index+1), row[index], gini))\n",
    "        if gini < b_score:\n",
    "            b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "    return {'index':b_index, 'value':b_value, 'groups':b_groups}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def to_terminal(group):\n",
    "    '''\n",
    "    create a terminal node value\n",
    "    '''\n",
    "    outcomes = [row[-1] for row in group]\n",
    "    return max(set(outcomes), key=outcomes.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def split(node, max_depth, min_size, depth):\n",
    "    '''\n",
    "    create child splits for a node or make terminal\n",
    "    '''\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    # check for a no split\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    # process left child\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left)\n",
    "        split(node['left'], max_depth, min_size, depth+1)\n",
    "    # process right child\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right)\n",
    "        split(node['right'], max_depth, min_size, depth+1) \n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def build_tree(train, max_depth, min_size):\n",
    "    '''\n",
    "    takes data and two hyperparameter\n",
    "    '''\n",
    "    root = get_split(train)\n",
    "    split(root, max_depth, min_size, 1)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def print_tree(node, depth=0):\n",
    "    '''\n",
    "    build a decision tree\n",
    "    '''\n",
    "    if isinstance(node, dict):\n",
    "        print('%s[X%d < %.3f]' % ((depth*' ', (node['index']+1), node['value'])))\n",
    "        print_tree(node['left'], depth+1)\n",
    "        print_tree(node['right'], depth+1)\n",
    "    else:\n",
    "        print('%s[%s]' % ((depth*' ', node)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def predict(node, row):\n",
    "    '''\n",
    "    takes a tree and uses it to predict an outcome for a given set of factors\n",
    "    '''\n",
    "    if row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we can try it out on a dataset of our own making: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dataset = [\n",
    "          [ 2.77, 1.78, 0],\n",
    "          [ 1.72, 1.16, 0],\n",
    "          [ 3.67, 2.81, 0],\n",
    "          [ 3.96, 2.61, 0],\n",
    "          [ 2.99, 2.20, 0],\n",
    "          [ 7.49, 3.16, 1],\n",
    "          [ 9.03, 3.33, 1],\n",
    "          [ 7.44, 0.47, 1],\n",
    "          [10.12, 3.23, 1],\n",
    "          [ 6.64, 3.31, 1]\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "split_data = get_split(dataset)\n",
    "print('Split: [X%d < %.3f]' % ((split_data['index']+1), split_data['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tree = build_tree(dataset, 1, 1)\n",
    "print_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we can test out the tree and see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for row in dataset:\n",
    "    prediction = predict(tree, row)\n",
    "    print('Predicted = %d,   Actual = %d' % (prediction,row[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It performs perfectly on our contrived data! What a surprise. \n",
    "\n",
    "The key takeaway is how it chose the better split of the two possible variables to split on, and stopped because there was a value in X1 that split the entire dataset accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pros and Cons of Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Advantages:\t\t\t\t\n",
    "1. Making predictions is fast, since the calculation process is not complicated (computationally efficient).\n",
    "2. It’s easy to understand what variables are important in prediction (look at the tree). They are among the easiest to visualize of ML models. They are intuitive and not hard to explain, even to someone with little econometrics training.\n",
    "3. If some data is missing, we might not be able to go all the way down the tree to a leaf, but we can still make a prediction by averaging all the leaves in the subtree we do reach. Further, they don't have the same problems with non-numerical or categorical data and collinearity\n",
    "5. There are fast, reliable algorithms to learn these trees \t\n",
    "\n",
    "Disadvantages:\n",
    "On the downside, they often don't have the highest accuracy in prediction and can be sensitive to minor changes in data. One way to overcome these weaknesses is to use multiple decision trees aggregated (random forests, boosting) or in conjunction with other models (stacking)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "#### Housing Prices: A Kaggle Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here’s an example using regression tree. Suppose you are interested in predicting home prices based on home characteristics. This could be because you are constructing, buying, or selling a home, and are looking for a ballpark price range based on home characteristics. Or perhaps you are investing in real estate and would like data to decide whether or not the home is above or below average price given its specific features. Alternatively, you could be interested in home value appraisal for taxation purposes. Your fundamental question: given individual housing characteristics, how much will this home sell for? \n",
    "\n",
    "\n",
    "Here, we have a dataset from Kaggle that includes all the relevant information to answer this question. The dataset has detailed information on a large number of housing characteristics and the sale price. Now, we can use regression tree to predict future home sale values. The following notebook demonstrates how to construct such a regression tree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The training dataset contains 1460 observations and 80 features. Let's start by calling packages needed for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-9b774b8c5e0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# Other Packages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "# Core Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# ML Packages\n",
    "from sklearn.linear_model import SGDRegressor, ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, f1_score, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split, learning_curve, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# ML Packages\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, f1_score, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, learning_curve, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Other Packages\n",
    "import graphviz \n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "After downloading necessary packages, we divide our sample into our training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "train_location = \"train.csv\"\n",
    "test_location = \"test.csv\"\n",
    "\n",
    "train = pd.read_csv(train_location)\n",
    "test = pd.read_csv(test_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To better understand what our data looks like, we look at a small subset of the training data to understand our data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We'll remove 'SalePrice' and 'Id' from the training dataset and log-transform 'SalePrice', which is our target variable of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "target = train['SalePrice']\n",
    "target_transformed = np.log(target)\n",
    "\n",
    "train = train.drop(['SalePrice', 'Id'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1. Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Before we start cleaning, let's develop a better understanding of what the data looks like. It looks like we have information about almost every aspect of a home (and its surrounding property) you could imagine, from commonly cited measures like square-feet and number of bedrooms to more detailed  metrics like the height of the basement or the masonry veneer type. Note that the final column is 'SalePrice', which is the variable we seek to predict. \n",
    "\n",
    "Below is a categorization of the features within the following categories: Sales, General, Location, Property, Interior, Basement, Utilities, Garage, and Exterior. This categorization is a subjective exercise, but it allowed me to become more familiar with the features and create general buckets within the dataset.  \n",
    "\n",
    "**Sale**\n",
    "- SalePrice: the property's sale price in dollars\n",
    "- MoSold: Month Sold\n",
    "- YrSold: Year Sold\n",
    "- SaleType: Type of sale\n",
    "- SaleCondition: Condition of sale\n",
    "\n",
    "**General**\n",
    "- MSSubClass: The building class\n",
    "- MSZoning: The general zoning classification\n",
    "- BldgType: Type of dwelling\n",
    "- HouseStyle: Style of dwelling\n",
    "- OverallQual: Overall material and finish quality\n",
    "- OverallCond: Overall condition rating\n",
    "- YearBuilt: Original construction date\n",
    "- YearRemodAdd: Remodel date\n",
    "- MiscFeature: Miscellaneous feature not covered in other categories\n",
    "- MiscVal: Dollar Value of miscellaneous feature\n",
    "\n",
    "**Location**\n",
    "- Street: Type of road access\n",
    "- Alley: Type of alley access\n",
    "- Neighborhood: Physical locations within Ames city limits\n",
    "- Condition1: Proximity to main road or railroad\n",
    "- Condition2: Proximity to main road or railroad (if a second is present)\n",
    "- LotFrontage: Linear feet of street connected to property\n",
    "\n",
    "**Property**\n",
    "- LotArea: Lot size in square feet\n",
    "- LotShape: General shape of property\n",
    "- LandContour: Flatness of the property\n",
    "- LotConfig: Lot configuration\n",
    "- LandSlope: Slope of property\n",
    "\n",
    "**Interior**\n",
    "- 1stFlrSF: First Floor square feet\n",
    "- 2ndFlrSF: Second floor square feet\n",
    "- LowQualFinSF: Low quality finished square feet (all floors)\n",
    "- GrLivArea: Above grade (ground) living area square feet\n",
    "- FullBath: Full bathrooms above grade\n",
    "- HalfBath: Half baths above grade\n",
    "- Bedroom: Number of bedrooms above basement level\n",
    "- Kitchen: Number of kitchens\n",
    "- KitchenQual: Kitchen quality\n",
    "- TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n",
    "- Functional: Home functionality rating\n",
    "- Fireplaces: Number of fireplaces\n",
    "- FireplaceQu: Fireplace quality\n",
    "\n",
    "**Basement**\n",
    "- BsmtQual: Height of the basement\n",
    "- BsmtCond: General condition of the basement\n",
    "- BsmtExposure: Walkout or garden level basement walls\n",
    "- BsmtFinType1: Quality of basement finished area\n",
    "- BsmtFinSF1: Type 1 finished square feet\n",
    "- BsmtFinType2: Quality of second finished area (if present)\n",
    "- BsmtFinSF2: Type 2 finished square feet\n",
    "- BsmtUnfSF: Unfinished square feet of basement area\n",
    "- TotalBsmtSF: Total square feet of basement area\n",
    "- BsmtFullBath: Basement full bathrooms\n",
    "- BsmtHalfBath: Basement half bathrooms\n",
    "\n",
    "**Utilities**\n",
    "- Utilities: Type of utilities available\n",
    "- Heating: Type of heating\n",
    "- HeatingQC: Heating quality and condition\n",
    "- CentralAir: Central air conditioning\n",
    "- Electrical: Electrical system\n",
    "\n",
    "**Garage**\n",
    "- GarageType: Garage location\n",
    "- GarageYrBlt: Year garage was built\n",
    "- GarageFinish: Interior finish of the garage\n",
    "- GarageCars: Size of garage in car capacity\n",
    "- GarageArea: Size of garage in square feet\n",
    "- GarageQual: Garage quality\n",
    "- GarageCond: Garage condition\n",
    "\n",
    "**Exterior**\n",
    "- RoofStyle: Type of roof\n",
    "- RoofMatl: Roof material\n",
    "- Exterior1st: Exterior covering on house\n",
    "- Exterior2nd: Exterior covering on house (if more than one material)\n",
    "- MasVnrType: Masonry veneer type\n",
    "- MasVnrArea: Masonry veneer area in square feet\n",
    "- ExterQual: Exterior material quality\n",
    "- ExterCond: Present condition of the material on the exterior\n",
    "- Foundation: Type of foundation\n",
    "- PavedDrive: Paved driveway\n",
    "- WoodDeckSF: Wood deck area in square feet\n",
    "- OpenPorchSF: Open porch area in square feet\n",
    "- EnclosedPorch: Enclosed porch area in square feet\n",
    "- 3SsnPorch: Three season porch area in square feet\n",
    "- ScreenPorch: Screen porch area in square feet\n",
    "- PoolArea: Pool area in square feet\n",
    "- PoolQC: Pool quality\n",
    "- Fence: Fence quality\n",
    "\n",
    "Note that these features are a mix of continuous (Lot Area, Year Built, Bedrooms) and categorical (House Style, Roof Style, Garage Type) variables. \n",
    "\n",
    "Let's start cleaning by checking for missing values. Below we find the number of missing values for each feature, for features with missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Find the number of missing values for each feature, including only those greater than 0. \n",
    "missing_values = pd.DataFrame(train.isnull().sum())\n",
    "missing_values = missing_values[(missing_values > 0).any(axis=1)]\n",
    "\n",
    "# Sort the values in ascending order. \n",
    "missing_values = missing_values.sort_values(by = 0, ascending = False)\n",
    "missing_values.columns = ['Number of Missing Values']\n",
    "\n",
    "# Calculate 'Percent Missing'\n",
    "missing_values['Percent Missing'] = missing_values['Number of Missing Values']/len(train)\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "19 of the 80 features are missing 1 or more value. However, the degree to which values are missing varies widely across the 19 variables. Only 7 of the 1460 properties have information about pool quality ('PoolQC') while only 1 property is missing information about the property's electrical system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We'll drop 'Alley', 'FireplaceQu', 'PoolQC', 'PoolArea', 'Fence', and 'MiscFeature' from our dataset, since most observations do not have information for those variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "train = train.drop(['MiscFeature', 'Fence', 'PoolQC', 'PoolArea', 'FireplaceQu', 'Alley'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "What about the others? Let's fill them in with the average of the feature if the feature is continuous or with the mode if the feature is categorical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "for feature in train:\n",
    "   # Features with a 'dtype' of O are categorical \n",
    "    if train[feature].dtype == 'O':\n",
    "       train[feature] = train[feature].fillna(train[feature].mode()[0])\n",
    "\n",
    "for feature in train:\n",
    "   # Features with a 'dtype' of i or are floats are continuous\n",
    "    if train[feature].dtype == np.float64 or train[feature].dtype == 'i':\n",
    "       train[feature] = train[feature].fillna(train[feature].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's confirm there aren't any remaining missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Should return 'False'\n",
    "train.isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We next look at outliers. To start, we'll explicitly determine which of our features are categorical and which are continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Create two empty lists\n",
    "continuous_features = []\n",
    "categorical_features = []\n",
    "\n",
    "# Seperate features by dtype\n",
    "for feature in train.columns:\n",
    "    if train[feature].dtype == \"object\":\n",
    "        categorical_features.append(feature)\n",
    "    else:\n",
    "        continuous_features.append(feature)\n",
    "        \n",
    "print(\"Number of Continuous Features:\", len(continuous_features), \"\\nNumber of Categorical Features:\", len(categorical_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We'll use this to filter outliers according to a simple rule: \n",
    "\n",
    "For each column we compute the z-score of each value in the column relative to the column mean and standard deviation. Since the direction of the difference is irrelevant, we take the absolute value. Here we remove rows that contain a (continuous) feature value greater than 5 standard deviations away from the standardized mean. \n",
    "\n",
    "This code below was adapted from [this](https://stackoverflow.com/questions/23199796/detect-and-exclude-outliers-in-pandas-dataframe) Stack Overflow article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "n_std = 5\n",
    "len(train) - len(train[train[continuous_features].apply(lambda x: np.abs(x - x.mean()) / x.std() < n_std).all(axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In doing so we drop 86 rows of our training data. We can adjust this threshold later to see if it affects our mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Drop rows in training set (and target) according to the rule described above\n",
    "target_transformed = target_transformed[train[continuous_features].apply(lambda x: np.abs(x - x.mean()) / x.std() < 10).all(axis=1)]\n",
    "train = train[train[continuous_features].apply(lambda x: np.abs(x - x.mean()) / x.std() < 10).all(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The final step of the cleaning process is to create dummy variables for the categorical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "train_no_dummies = train\n",
    "train = pd.get_dummies(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We apply the same changes we made, cleaning missing values, checking for outliers, and getting dummies for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "test = test.drop(['MiscFeature', 'Fence', 'PoolQC', 'FireplaceQu', 'Alley'], axis = 1)\n",
    "\n",
    "for feature in test:\n",
    "    # Features with a 'dtype' of O are categorical \n",
    "   if test[feature].dtype == 'O':\n",
    "       test[feature] = test[feature].fillna(test[feature].mode()[0])\n",
    "for feature in test:\n",
    "    # Features with a 'dtype' of i or are floats are continuous\n",
    "    if test[feature].dtype == np.float64 or test[feature].dtype == 'i':\n",
    "       test[feature] = test[feature].fillna(test[feature].mean())\n",
    "\n",
    "# Only keep columns in test that are also found in train\n",
    "test = test.reindex(columns = train.columns, fill_value=0)\n",
    "\n",
    "test_no_dummies = test\n",
    "test = pd.get_dummies(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2. Data Exploration & Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With the data cleaned we're now ready to explore the data. We begin by calculating the correlations for all of the continuous features and ranking them from -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Filter out categorical variables\n",
    "values = []\n",
    "df = train[continuous_features]\n",
    "\n",
    "# Iterate over each continous feature and calcualte its correlation with the target\n",
    "for feature in df.columns:\n",
    "    values.append([feature, df[feature].corr(target_transformed)])\n",
    "    \n",
    "# Sort the values and present them in a Pandas Dataframe\n",
    "values = sorted(values, key=lambda x: x[1])\n",
    "correlations = pd.DataFrame(values, columns = ['Feature', 'Correlation with SalePrice'])\n",
    "correlations.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It looks like 'OverallQual', 'GrLivArea' 'GarageCars', 'GarageArea', 'TotalBsmtSF' and '1stFlrSF' are moderately to highly correlated with 'SalePrice'. We'll need to examine the coefficients on these features when we do model fitting later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3. Model Fitting & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In our modeling, we will use the technigues described in the analytical framework to estimate a regression tree model that estimates the log of hose prices for homes in the Kaggle dataset. We will estimate both a full regression tree that includes all possible variables, as well as a simpler regression tree that gives a better visual and conceptual representation of how regression trees work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Small Regression Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We first estimate a tree with a subset of the variables to show a concise and easy to understand example of how regression trees work. In particular, we use a subset of variables that are likely to be the most salient for homebuyers to consider when purchasing a home. The variables we include are total square footage, overall quality, overall condition, lot size, the year the home was built, as well as the number of bedrooms and bathrooms. We subset both the training data and the test data by these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare the training data\n",
    "s_train = train_no_dummies\n",
    "s_train['TotalSF'] = train_no_dummies['TotalBsmtSF'] + train_no_dummies['1stFlrSF'] + train_no_dummies['2ndFlrSF']\n",
    "s_train = s_train[['TotalSF', 'OverallQual', 'OverallCond', 'LotArea', 'YearBuilt', 'BedroomAbvGr', 'FullBath', 'HalfBath']]\n",
    "\n",
    "# Prepare the test data\n",
    "s_test = test_no_dummies\n",
    "s_test['TotalSF'] = test_no_dummies['TotalBsmtSF'] + test_no_dummies['1stFlrSF'] + test_no_dummies['2ndFlrSF']\n",
    "s_test = s_test[['TotalSF', 'OverallQual', 'OverallCond', 'LotArea', 'YearBuilt', 'BedroomAbvGr', 'FullBath', 'HalfBath']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We first scale the test data and the training data to prepare it for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Scale the training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(s_train)\n",
    "scaled_s_train_df = scaler.transform(s_train)\n",
    "\n",
    "# Scale the test data\n",
    "scaler.fit(s_test)\n",
    "scaled_s_test_df = scaler.transform(s_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We fit our model with our training data to a regression tree with a maximum depth of 3, and we generate a set of predictions for both our training data and our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(s_train, target_transformed, test_size=0.33, random_state=42)\n",
    "\n",
    "clf = DecisionTreeRegressor(max_depth = 3)  \n",
    "clf = clf.fit(X_train, y_train)\n",
    "train_predictions = clf.predict(X_train)\n",
    "test_predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We create a visual representation of this simplified regression tree using the \"graphviz\" package. The visualization shows the leaves and branches of our regression tree model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\n",
    "dot_data = tree.export_graphviz(clf, out_file = None, feature_names = s_train.columns, label = 'root', filled = True, impurity = True, proportion = True, rounded = True)\n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We also report a portion of our results, including the actual sale price and the predicted sale price for the first 20 observations in our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Full Regression Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now using the complete dataset, we again begin by scaling the training and test data and confirming that the matrices have the correct shape. If the data sets are correctly shaped, the training and test sets should have the same number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Scale the training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train)\n",
    "scaled_train_df = scaler.transform(train)\n",
    "\n",
    "# Scale the test data\n",
    "scaler.fit(test)\n",
    "scaled_test_df = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If the data sets are correctly shaped, the training and test sets should have the same number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(target_transformed.shape, scaled_train_df.shape, scaled_test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We prepare our testing and training datasets for our regression tree model, and we determine the best parameters to use in our analysis using a grid search cross-validation method. We also fit our model to the training data, and generate predictions for both the training data and the test data. We also report what hyperparameters we use based on our cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(scaled_train_df, target_transformed, test_size=0.33, random_state=42)\n",
    "\n",
    "param_dist = {\"min_samples_leaf\": [3, 5, 8], \"max_depth\": [15, 20, 25, 30]}\n",
    "model = DecisionTreeRegressor()\n",
    "dt = GridSearchCV(model, param_grid=param_dist, scoring='neg_mean_squared_error')\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "dt_train_predictions = dt.predict(X_train)\n",
    "dt_test_predictions = dt.predict(X_test)\n",
    "print(\"Best Params: {}\".format(dt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\"Actual\": np.exp(y_train), \"Predicted\": list(np.exp(clf.predict(X_train)))})\n",
    "results = results.reset_index(drop=True)\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Reference 1: Page 325 describes Regression Tree\n",
    "     http://web.b.ebscohost.com/ehost/pdfviewer/pdfviewer?vid=1&sid=76d6816d-9bab-4ef7-b763-0e3a8872b6fe%40sessionmgr103\n",
    "\n",
    "Link 2: http://web.b.ebscohost.com/ehost/detail/detail?vid=0&sid=76d6816d-9bab-4ef7-b763      \n",
    "     0e3a8872b6fe%40sessionmgr103&bdata=JnNpdGU9ZWhvc3QtbGl2ZSZzY29wZT1zaXRl#AN=2009-22665-002&db=pdh\n",
    "\n",
    "Strobl, C.; Malley, J.; Tutz, G. (2009). \"An Introduction to Recursive Partitioning: Rationale, Application and Characteristics      of Classification and Regression Trees, Bagging and Random Forests\". Psychological Methods. 14 (4): 323–348.                    doi:10.1037/a0016973.\n",
    "\n",
    "Reference 2: Chipman, Hugh A., Edward I. George, and Robert E. McCulloch. \"Bayesian CART model search.\" Journal of the American      Statistical Association 93.443 (1998): 935-948. https://search.proquest.com/docview/274825524?pq-origsite=gscholar\n",
    "\n",
    "Reference 3: “Decision Tree Learning.” Wikipedia, Wikimedia Foundation, 12 Apr. 2018, \n",
    "     en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
